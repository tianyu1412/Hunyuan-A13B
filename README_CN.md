<p align="left">
   <a href="README.md">English</a>  ï½œ ä¸­æ–‡</a>&nbsp
</p>
<br><br>

<p align="center">
 <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br>
</p><p></p>

<p align="center">
    ğŸ«£&nbsp;<a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ–¥ï¸&nbsp;<a href="https://llm.hunyuan.tencent.com/" style="color: red;"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•–&nbsp;<a href="https://cloud.tencent.com/product/hunyuan"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•¹ï¸&nbsp;<a href="https://hunyuan.tencent.com/?model=hunyuan-a13b"><b>Demo</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    <img src="https://avatars.githubusercontent.com/u/109945100?s=200&v=4" width="16"/>&nbsp;<a href="https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct"><b>ModelScope</b></a>
</p>

<p align="center">
    <a href="https://github.com/Tencent/Hunyuan-A13B"><b>GITHUB</b></a>
</p>




## æ¨¡å‹ä»‹ç»

éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œç§‘å­¦ä»»åŠ¡ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„æ‰©å¤§ï¼Œå¦‚ä½•åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–èµ„æºæ¶ˆè€—æˆä¸ºä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œå½“å‰äº®ç›¸çš„ Hunyuan-A13B æ¨¡å‹ï¼Œæ‹¥æœ‰800äº¿æ€»å‚æ•°å’Œ130äº¿æ¿€æ´»å‚æ•°ã€‚ä¸ä»…åœ¨æ•ˆæœä¸Šè¾¾åˆ°äº†é«˜æ ‡å‡†ï¼Œè€Œä¸”åœ¨å°ºå¯¸ä¸Šä¹Ÿåšåˆ°äº†æè‡´çš„ä¼˜åŒ–ï¼ŒæˆåŠŸå¹³è¡¡äº†æ¨¡å‹æ€§èƒ½ä¸èµ„æºå ç”¨ã€‚


### æ ¸å¿ƒç‰¹æ€§ä¸ä¼˜åŠ¿
- â€‹**å°å‚æ•°é‡ï¼Œé«˜æ€§èƒ½**â€‹ï¼šä»…æ¿€æ´»130äº¿å‚æ•°ï¼ˆæ€»å‚æ•°é‡800äº¿ï¼‰ï¼Œå³å¯åœ¨å¤šæ ·åŒ–åŸºå‡†ä»»åŠ¡ä¸­åª²ç¾æ›´å¤§è§„æ¨¡æ¨¡å‹çš„ç«äº‰åŠ›è¡¨ç° 
- â€‹**æ··åˆæ¨ç†æ”¯æŒ**â€‹ï¼šåŒæ—¶æ”¯æŒå¿«æ€è€ƒå’Œæ…¢æ€è€ƒä¸¤ç§æ¨¡å¼ï¼Œæ”¯æŒç”¨æˆ·çµæ´»é€‰æ‹© 
- â€‹**è¶…é•¿ä¸Šä¸‹æ–‡ç†è§£**â€‹ï¼šåŸç”Ÿæ”¯æŒ256Kä¸Šä¸‹æ–‡çª—å£ï¼Œåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­ä¿æŒç¨³å®šæ€§èƒ½
- â€‹**å¢å¼ºAgentèƒ½åŠ›**â€‹ï¼šä¼˜åŒ–Agentèƒ½åŠ›ï¼Œåœ¨BFCL-v3ã€Ï„-Benchç­‰æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•ä¸­é¢†å…ˆ
- â€‹**é«˜æ•ˆæ¨ç†**â€‹ï¼šé‡‡ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ç­–ç•¥ï¼Œæ”¯æŒå¤šé‡åŒ–æ ¼å¼ï¼Œå®ç°é«˜æ•ˆæ¨ç†
    

### ä¸ºä½•é€‰æ‹©Hunyuan-A13Bï¼Ÿ
ä½œä¸ºå…¼å…·å¼ºå¤§æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡çš„å¤§æ¨¡å‹ï¼ŒHunyuan-A13Bæ˜¯ç ”ç©¶è€…ä¸å¼€å‘è€…åœ¨èµ„æºå—é™æ¡ä»¶ä¸‹è¿½æ±‚é«˜æ€§èƒ½çš„ç†æƒ³é€‰æ‹©ã€‚æ— è®ºå­¦æœ¯ç ”ç©¶ã€é«˜æ€§ä»·æ¯”AIè§£å†³æ–¹æ¡ˆå¼€å‘ï¼Œè¿˜æ˜¯åˆ›æ–°åº”ç”¨æ¢ç´¢ï¼Œæœ¬æ¨¡å‹éƒ½èƒ½æä¾›å¼ºå¤§çš„åŸºç¡€æ”¯æŒã€‚


&nbsp;

## æ–°é—»
<br>

* 2025.6.26 æˆ‘ä»¬åœ¨Hugging Faceå¼€æºäº† **Hunyuan-A13B-Instruct**ï¼Œ**Hunyuan-A13B-Pretrain**, **Hunyuan-A13B-Instruct-FP8**ï¼Œ **Hunyuan-A13B-Instruct-GPTQ-Int4**ã€‚å¹¶å‘å¸ƒäº†æŠ€æœ¯æŠ¥å‘Šå’Œè®­ç»ƒæ¨ç†æ“ä½œæ‰‹å†Œï¼Œè¯¦ç»†ä»‹ç»äº†æ¨¡å‹èƒ½åŠ›å’Œè®­ç»ƒä¸æ¨ç†çš„æ“ä½œã€‚

## æ¨¡å‹ç»“æ„

Hunyuan-A13Bé‡‡ç”¨äº†ç»†ç²’åº¦æ··åˆä¸“å®¶ï¼ˆFine-grained Mixture of Expertsï¼ŒFine-grained MoEï¼‰æ¶æ„ï¼ŒåŒ…å«800äº¿å‚æ•°å’Œ130äº¿æ¿€æ´»å‚æ•°ï¼Œç´¯è®¡è®­ç»ƒäº†è¶…è¿‡ 20T tokensã€‚è¯¥æ¨¡å‹æ”¯æŒ 256K çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥ä¸‹ä¸ºæ¨¡å‹ç»“æ„ç»†èŠ‚:
* æ€»å‚æ•°: 80B
* æ¿€æ´»å‚æ•°: 13B
* å±‚æ•°: 32
* Attention Heads: 32
* å…±äº«ä¸“å®¶æ•°: 1
* éå…±äº«ä¸“å®¶æ•°: 64
* è·¯ç”±ç­–ç•¥: Top-8
* æ¿€æ´»å‡½æ•°: SwiGLU
* éšå±‚ç»´åº¦: 4096
* ä¸“å®¶éšå±‚ç»´åº¦: 3072 

## Benchmarkè¯„ä¼°æ¦œå• 

**Hunyuan-A13B-Pretrain** åœ¨ 12/14 ä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†Hunyuanä¸Šä¸€ä»£52Bæ¿€æ´»å‚æ•°çš„MoEæ¨¡å‹Hunyuan-Largeï¼Œè¯å®äº†å®ƒåœ¨é¢„è®­ç»ƒä»»åŠ¡ä¸Šå‡ºè‰²çš„èƒ½åŠ›ã€‚ä¸ä¸šç•Œæ›´å¤§å‚æ•°é‡çš„Denseå’ŒMoEæ¨¡å‹ç›¸æ¯”, Hunyuan-A13Båœ¨å¤šä¸ªä»£ç å’Œæ•°å­¦ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æœ€é«˜åˆ†æ•°ã€‚åœ¨MMLU, MMLU-PROç­‰è¯¸å¤šä¼—èšåˆä»»åŠ¡ä¸Š, Hunyuan-A13Bè¾¾åˆ°äº†ä¸Qwen3-A22Bæ¨¡å‹åŒç­‰çš„æ°´å¹³ï¼Œè¡¨ç°å‡ºä¼˜ç§€çš„ç»¼åˆèƒ½åŠ›ã€‚

| Model            | Hunyuan-Large | Qwen2.5-72B  | Qwen3-A22B | Hunyuan-A13B |
|------------------|---------------|--------------|-------------|---------------|
| MMLU             | 88.40          | 86.10         | 87.81        | 88.17          |
| MMLU-Pro         | 60.20          | 58.10        | 68.18           | 67.23          |
| MMLU-Redux              |  87.47         | 83.90         | 87.40        | 87.67          |
| BBH        | 86.30             | 85.80            | 88.87        | 87.56          |
| SuperGPQA    |  38.90         | 36.20          | 44.06           | 41.32          |
| EvalPlus       | 75.69          | 65.93         | 77.60        | 78.64          |
| MultiPL-E             | 59.13             | 60.50            | 65.94        | 69.33          |
| MBPP | 72.60             | 76.00            | 81.40        | 83.86          |
| CRUX-I             | 57.00          | 57.63          | -        | 70.13          |
| CRUX-O             | 60.63          | 66.20          | 79.00        | 77.00          |
| MATH            | 69.80          | 62.12         | 71.84        | 72.35          |
| CMATH            | 91.30          | 84.80         | -        | 91.17          |
| GSM8k         | 92.80             | 91.50           | 94.39        | 91.83          |
| GPQA            | 25.18             | 45.90            | 47.47        | 49.12          |

**Hunyuan-A13B-Instruct** åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æå…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦ã€ç§‘å­¦ã€agentç­‰é¢†åŸŸã€‚æˆ‘ä»¬ä¸ä¸€äº›å¼ºåŠ›æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚

| Topic               | Bench                         | OpenAI-o1-1217 | DeepSeek R1 | Qwen3-A22B | Hunyuan-A13B-Instruct |
|:-------------------:|:-----------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|
| **Mathematics**     | AIME 2024<br>AIME 2025<br>MATH | 74.3<br>79.2<br>96.4 | 79.8<br>70<br>94.9 | 85.7<br>81.5<br>94.0 | 87.3<br>76.8<br>94.3 |
| **Science**         | GPQA-Diamond<br>OlympiadBench | 78<br>83.1 | 71.5<br>82.4 | 71.1<br>85.7 | 71.2<br>82.7 |
| **Coding**          | Livecodebench<br>Fullstackbench<br>ArtifactsBench | 63.9<br>64.6<br>38.6 | 65.9<br>71.6<br>44.6 | 70.7<br>65.6<br>44.6 | 63.9<br>67.8<br>43 |
| **Reasoning**       | BBH<br>DROP<br>ZebraLogic    | 80.4<br>90.2<br>81 | 83.7<br>92.2<br>78.7 | 88.9<br>90.3<br>80.3 | 89.1<br>91.1<br>84.7 |
| **Instruction<br>Following** | IF-Eval<br>SysBench  | 91.8<br>82.5 | 88.3<br>77.7 | 83.4<br>74.2 | 84.7<br>76.1 |
| **Text<br>Creation**| LengthCtrl<br>InsCtrl       | 60.1<br>74.8 | 55.9<br>69 | 53.3<br>73.7 | 55.4<br>71.9 |
| **NLU**             | ComplexNLU<br>Word-Task     | 64.7<br>67.1 | 64.5<br>76.3 | 59.8<br>56.4 | 61.2<br>62.9 |
| **Agent**           | BDCL v3<br> Ï„-Bench<br>ComplexFuncBench<br> C3-Bench | 67.8<br>60.4<br>47.6<br>58.8 | 56.9<br>43.8<br>41.1<br>55.3 | 70.8<br>44.6<br>40.6<br>51.7 | 78.3<br>54.7<br>61.2<br>63.5 |


## æ•°æ®

Hunyuan-A13B æä¾›äº†æ¨¡å‹è®­ç»ƒç›¸å…³æµç¨‹ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤ç« èŠ‚å¯¹è®­ç»ƒæ•°æ®æ ¼å¼è¿›è¡Œå¤„ç†ä»¥ä¾›æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚

### è®­ç»ƒæ•°æ®æ ¼å¼åŠå¤„ç† 

**Hunyuan-A13B-Instruct åŒæ—¶æ”¯æŒæ…¢æ€è€ƒä¸å¿«æ€è€ƒä¸¤ç§æ¨¡å¼ï¼Œæ¨¡å‹çš„é»˜è®¤è¾“å‡ºæ˜¯æ…¢æ€è€ƒæ¨¡å¼ï¼Œè‹¥æƒ³è®©æ¨¡å‹è¿›è¡Œå¿«æ€è€ƒï¼Œå¯åœ¨ query å‰é™„åŠ ä¸Š "/no_think"ã€‚**

è®­ç»ƒæ•°æ®æŒ‰ç…§ä»¥ä¸‹å½¢å¼å¤„ç†ä¸º messages æ ¼å¼ï¼Œè®­ç»ƒå’Œæ¨ç†çš„é»˜è®¤ system prompt ä¸ºç©ºï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œè®¾å®šã€‚

```python
# think_pattern
think = ""
answer = ""
think_pattern = f"<think>\n{think}\n</think>\n<answer>\n{answer}\n</answer>"

# fast think pattern
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "/no_thinkæµ·æ°´ä¸ºä»€ä¹ˆæ˜¯å’¸çš„" },
    {"role": "assistant", "content": "<think>\n\n</think>\n<answer>\næµ·æ°´æ˜¯å’¸çš„ä¸»è¦æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰è®¸å¤šæº¶è§£åœ¨æ°´ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨ã€‚è¿™äº›ç›ç±»å’ŒçŸ¿ç‰©è´¨æ¥è‡ªäºåœ°çƒè¡¨é¢çš„å²©çŸ³å’ŒåœŸå£¤ä¸­çš„åŒ–å­¦ç‰©è´¨ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä»¬è¢«å¸¦åˆ°äº†æµ·æ´‹ä¸­ã€‚å½“æµ·æ°´è’¸å‘æ—¶ï¼Œæ°´åˆ†è’¸å‘æ‰äº†ï¼Œä½†ç›ç±»å’ŒçŸ¿ç‰©è´¨ä»ç„¶ç•™åœ¨æ°´ä¸­ï¼Œå¯¼è‡´æµ·æ°´å˜å¾—æ›´åŠ å’¸å‘³ã€‚å› æ­¤ï¼Œæµ·æ°´çš„å’¸åº¦æ˜¯ç”±å…¶ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨çš„å«é‡å†³å®šçš„ã€‚\n</answer>"}
]

# slow think pattern
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "1+1=" },
    {"role": "assistant", "content": "<think>\nå—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯1åŠ 1ç­‰äºå¤šå°‘ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤è¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„ç®—æœ¯é—®é¢˜ã€‚1åŠ 1åœ¨åè¿›åˆ¶çš„æ•°å­¦ä½“ç³»ä¸­ï¼Œé€šå¸¸çš„ç»“æœæ˜¯2ã€‚ä¸è¿‡ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘æ˜¯å¦æœ‰å…¶ä»–æƒ…å†µï¼Œæ¯”å¦‚äºŒè¿›åˆ¶æˆ–è€…å…¶ä»–æ•°åˆ¶ï¼Œä½†ç”¨æˆ·æ²¡æœ‰ç‰¹åˆ«è¯´æ˜ï¼Œæ‰€ä»¥é»˜è®¤åº”è¯¥æ˜¯åè¿›åˆ¶ã€‚å¦å¤–ï¼Œæœ‰æ—¶å€™å¯èƒ½ä¼šæœ‰è„‘ç­‹æ€¥è½¬å¼¯çš„æƒ…å†µï¼Œæ¯”å¦‚åœ¨æŸäº›è¯­å¢ƒä¸‹1+1å¯èƒ½ç­‰äº1ï¼ˆæ¯”å¦‚1æ»´æ°´åŠ 1æ»´æ°´è¿˜æ˜¯1æ»´æ°´ï¼‰ï¼Œä½†é€šå¸¸æ•°å­¦é—®é¢˜ä¸­éƒ½æ˜¯2ã€‚æ‰€ä»¥æœ€å‡†ç¡®çš„å›ç­”åº”è¯¥æ˜¯2ã€‚</think>\n<answer>\nåœ¨åè¿›åˆ¶çš„åŸºæœ¬ç®—æœ¯è¿ç®—ä¸­ï¼Œ1åŠ 1çš„ç»“æœæ˜¯2ã€‚è¿™æ˜¯æ•°å­¦ä¸­æœ€åŸºç¡€çš„åŠ æ³•è¿ç®—ä¹‹ä¸€ï¼Œéµå¾ªè‡ªç„¶æ•°çš„åŠ æ³•è§„åˆ™ã€‚å› æ­¤ï¼Œ1 + 1 = 2ã€‚\n</answer>"}
]

from tokenizers import ByteLevelBPETokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("./models", use_fast=False, trust_remote_code=True)
ids = tokenizer.apply_chat_template(messages)
```

&nbsp;

## å¿«é€Ÿå¼€å§‹

æ‚¨å¯ä»¥å‚ç…§<a href="examples/README.md">å¿«é€Ÿå¼€å§‹æ–‡æ¡£</a>ä¸­çš„å†…å®¹è¿›è¡Œå¿«é€Ÿä¸Šæ‰‹ã€‚

## æ¨¡å‹è®­ç»ƒ 

### ç¡¬ä»¶éœ€æ±‚

ç»è¿‡æµ‹è¯•ï¼Œä¸å¼€ make_moe_param_leaf_module ä»¥åŠ zero3+offloadï¼Œmax_seq_length ä¸º 2048ï¼Œå…¨é‡å¾®è°ƒæœ€å°‘éœ€è¦å•æœº 8 å¡ï¼ˆæ˜¾å­˜è‡³å°‘80Gï¼‰ã€‚

### å¯åŠ¨æ–¹å¼

å‚è€ƒï¼š[HuggingFace Transformers Trainer](https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer)

#### å•æœºå¯åŠ¨è®­ç»ƒ

åœ¨`train`ç›®å½•ä¸‹ï¼Œæ‰§è¡Œï¼š

```sh
pip install -r requirements.txt
bash train.sh
```

#### å¤šæœºå¯åŠ¨è®­ç»ƒ

å¦‚æœè¦ç”¨å¤šå°æœºå™¨å¯åŠ¨è®­ç»ƒï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼Œå¹¶ä¿è¯å¤šå°æœºå™¨åœ¨ä¸€ä¸ªé›†ç¾¤å†…ã€‚

##### é…ç½®æœºå™¨é—´å…å¯† ssh ç™»å½•

ä»¥ä¸‹æ“ä½œä»¥ä¸¤ä¸ªæœºå™¨ä¸ºä¾‹ï¼Œä¸¤å°æœºå™¨çš„ ip åˆ†åˆ«ä»¥`${ip1}`å’Œ`${ip2}`æ ‡è¯†ï¼Œä»¥ä¸‹æ“ä½œå‡åœ¨ docker container å†…æ‰§è¡Œã€‚

é¦–å…ˆï¼Œé…ç½®å¤šæœºcontainerå…å¯†ï¼Œåœ¨æ¯å°æœºå™¨ä¸Šæ‰§è¡Œã€‚

```sh
ssh-keygen			# ç”Ÿæˆid_rsaå’Œid_rsa.pubï¼Œç”¨äºå…å¯†ç™»å½•
ssh-keygen -t rsa -A    # ç”Ÿæˆ/etc/ssh/ssh_host_rsa_keyå’Œssh_host_ecdsa_keyï¼Œ ç”¨äºåé¢å¯åŠ¨ssh listen
/usr/sbin/sshd -p 36005 -o ListenAddress=0.0.0.0        # å¯åŠ¨Listen
echo "Port 36005" > ~/.ssh/config   # ssh è¿æ¥ç«¯å£ä¿®æ”¹ä¸º 36005
passwd root    # éœ€è¦é…ç½®rootå¯†ç ï¼Œå¦åˆ™ç›‘æµ‹å¹³å°ä¼šæŠ¥è­¦
```

æ³¨æ„ï¼šè¿™é‡Œçš„`36005`æ˜¯ä¸€ä¸ªç¤ºä¾‹ç«¯å£ï¼Œå¯ä»¥é€‰ç”¨ä»»æ„ç«¯å£ï¼Œä½†éœ€è¦ä¿è¯ä½¿ç”¨çš„ç«¯å£**å¼€æ”¾**ä¸”**ä¸è¢«å…¶ä»–çš„è¿›ç¨‹å ç”¨**ã€‚

æ¥ä¸‹æ¥ï¼Œåœ¨æ¯å°æœºå™¨çš„ container å†…ï¼Œæ‰§è¡Œï¼š

```sh
cat ~/.ssh/id_rsa.pub
```

**å°†è¾“å‡ºçš„ ssh å…¬é’¥å¤åˆ¶å¹¶ç²˜è´´åˆ°`~/.ssh/authorized_keys`æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œä¸€ä¸ªå…¬é’¥ï¼Œæ¯å°æœºå™¨ä¸Šéƒ½è¦åšè¿™ä¸ªæ“ä½œ**ã€‚æœ€ç»ˆæ¯å°æœºå™¨ä¸Šçš„`~/.ssh/authorized_keys`æ–‡ä»¶å†…å®¹åº”å½“æ˜¯ä¸€è‡´çš„ï¼Œå¹¶ä¸”åŒ…å«äº†æ‰€æœ‰æœºå™¨çš„å…¬é’¥ã€‚

éœ€è¦æ³¨æ„ï¼Œå¤šèŠ‚ç‚¹è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œçš„ä»£ç éƒ½å¾—ä¸€è‡´ï¼Œå»ºè®®æŒ‚è½½ä¸€ä¸ªå…±äº«çš„ç½‘ç»œç›˜ï¼Œå¦‚æœæ— æ³•æŒ‚è½½å…±äº«ç½‘ç›˜ï¼Œåˆ™éœ€è¦æ‰‹åŠ¨å°†æ•°æ®é›†ã€è„šæœ¬ã€ä»£ç å¤åˆ¶åœ¨å¤šå°æœºå™¨çš„ç›¸åŒç›®å½•ä¸‹ã€‚

##### å¯åŠ¨å¤šæœºè®­ç»ƒ

åœ¨ä»¥ä¸Šå‡†å¤‡æ­¥éª¤å‡†å¤‡å¥½äº†ä¹‹åï¼Œä»¥åŠç¡®è®¤ä¾èµ–å·²ç»å®‰è£…å®Œæˆï¼ˆå¦‚æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ`pip install -r requirements.txt`å®‰è£…ï¼‰ï¼Œå°±å¯ä»¥åœ¨`train.sh`ä¸­çš„å¼€å¤´å¢åŠ ä»¥ä¸‹é…ç½®ï¼š

```shell
export HOST_GPU_NUM=8
# å½“å‰æœºå™¨ip
export LOCAL_IP=${ip1}
# å¤šèŠ‚ç‚¹æœºå™¨ipï¼Œé€—å·éš”å¼€
export NODE_IP_LIST="${ip1}:8,${ip2}:8"
# æœºå™¨èŠ‚ç‚¹ä¸ªæ•°
export NODES=2
export NODE_NUM=$((${NODES} * ${HOST_GPU_NUM}))
```

æ³¨æ„ï¼šå°†ä»¥ä¸Šçš„`${ip1}`å’Œ`${ip2}`æ›¿æ¢ä¸ºçœŸå®çš„ ip åœ°å€ï¼

ç„¶åï¼Œåœ¨`${ip1}`çš„æœºå™¨ä¸Šï¼Œåœ¨`train/`ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ`bash train.sh`å³å¯ï¼Œæ³¨æ„ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶å¯èƒ½ä¼šçœ‹è§ä»¥ä¸‹çš„è¾“å‡ºï¼š

```ssh
The authenticity of host '[ip]:36005 ([ip]:36005)' can't be established.
ECDSA key fingerprint is xxxxxx.
ECDSA key fingerprint is MD5:xxxxxx.
Are you sure you want to continue connecting (yes/no)?
```

æ­¤æ—¶è¾“å…¥`yes`å³å¯ç»§ç»­ã€‚

##### å…³é”®å‚æ•°

è„šæœ¬ä¸­çš„å…³é”®å‚æ•°å¦‚ä¸‹ï¼š

- `--deepspeed`: æ­¤å‚æ•°åº”å½“æŒ‡å‘ä¸€ä¸ª deepspeed çš„é…ç½®æ–‡ä»¶ï¼Œ`train`æ–‡ä»¶å¤¹ä¸‹æä¾›äº†ä¸‰ç§ DeepSpeed çš„é»˜è®¤é…ç½®æ–‡ä»¶ï¼š`ds_zero2_no_offload.json`, `ds_zero3_no_offload.json`, `ds_zero3_offload.json`ï¼Œè¿™ä¸‰ä¸ªé…ç½®æ–‡ä»¶æ‰€éœ€æ˜¾å­˜ä¾æ¬¡å‡å°‘
- `--model_name_or_path`: è¦åŠ è½½çš„ HF é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œç¡®ä¿è¿™ä¸ªè·¯å¾„ä¸‹åŒ…å«äº† `modeling_hunyuan.py` å’Œ `configuration_hunyuan.py` æ–‡ä»¶ï¼Œå¦åˆ™æ— æ³•åŠ è½½
- `--tokenizer_name_or_path`: tokenizer æ–‡ä»¶å¤¹è·¯å¾„ï¼Œç¡®ä¿è¿™ä¸ªè·¯å¾„ä¸‹åŒ…å«äº†`tokenization_hy.py` æ–‡ä»¶ï¼Œå¦åˆ™æ— æ³•åŠ è½½
- `--train_data_file`: è®­ç»ƒæ–‡ä»¶è·¯å¾„ï¼Œåº”è¯¥ä¸ºä¸€ä¸ª jsonl æ–‡ä»¶
- `--output_dir`: è¾“å‡ºæ–‡ä»¶å¤¹ï¼Œlogã€tensorboard å’Œæƒé‡éƒ½ä¼šå­˜å‚¨åœ¨è¿™ä¸ªè·¯å¾„ä¸‹
- `--per_device_train_batch_size`: æ¯å¼ å¡ä¸Šçš„ batch size
- `--gradient_accumulation_steps`: æ¢¯åº¦ç´¯è®¡æ¬¡æ•°ï¼Œ`per_device_train_batch_size * gradient_accumulation_steps * dp_size`ä¸º global_batch_size
- `--max_steps`: è®­ç»ƒçš„æ€»æ­¥æ•°
- `--save_steps`: æ¯å¤šå°‘ä¸ª step å­˜å‚¨ä¸€ä¸ª checkpoint
- `--use_lora`: æ˜¯å¦ç”¨ lora è®­ç»ƒï¼ŒåŒæ—¶æ¥æ”¶`--lora_rank`ï¼Œ`--lora_alpha`å’Œ`--lora_dropout`å‚æ•°ã€‚lora é»˜è®¤åº”ç”¨äº "q_proj", "k_proj", "v_proj", "o_proj" å››ä¸ªå‚æ•°ï¼Œå¦‚æœéœ€è¦æ”¹å˜çš„è¯åœ¨ä»£ç ä¸­ä¿®æ”¹å³å¯ã€‚æ³¨æ„ï¼š**ä½¿ç”¨ lora è®­ç»ƒæ—¶ï¼Œåªä¼šä¿å­˜ lora çš„æƒé‡ï¼Œè€Œä¸ä¼šä¿å­˜ base æ¨¡å‹çš„æƒé‡**ï¼Œå¦‚æœéœ€è¦åˆå¹¶ lora æƒé‡ï¼Œçœ‹ä¸‹é¢çš„â€œLora æƒé‡åˆå¹¶â€ä¸€èŠ‚
- `--make_moe_param_leaf_module`ï¼šå½“ç”¨ zero3 ä»¥åŠ MoE è®­ç»ƒæ—¶ï¼Œå°† MoE æ¨¡å—è§†ä½œä¸€ä¸ª leaf moduleï¼Œå³å®ƒçš„å‚æ•°ä¸è¿›è¡Œ zero3 åˆ‡åˆ†ï¼Œè¿™ä¸ªé€‰é¡¹é¢„è®¡ä¼šæ˜¾è‘—å¢åŠ æ˜¾å­˜å ç”¨
- `--gradient_checkpointing`ï¼šå¼€å¯æ¢¯åº¦é‡è®¡ç®—
- `--train_attention_params_only`: æ˜¯å¦åªè®­ç»ƒ attention å‚æ•°
- `--learning_rate`: è®­ç»ƒæ—¶çš„æœ€å¤§å­¦ä¹ ç‡
- `--min_lr`: è®­ç»ƒæ—¶çš„æœ€å°å­¦ä¹ ç‡
- `--use_flash_attn`: å¼€å¯ flash-attention è¿›è¡Œè®­ç»ƒåŠ é€Ÿ

**æ³¨æ„ï¼š**

- å¦‚æœæƒ³ä»ä¸€ä¸ªä¸­é€”ä¿å­˜çš„ ckpt ç»§ç»­è®­ç»ƒï¼Œè€Œä¸æ˜¯åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„æƒé‡ï¼Œç›´æ¥æŒ‡å®š`--resume_from_checkpoint`ä¸ºä¹‹å‰è®­ç»ƒä¿å­˜çš„ ckpt è·¯å¾„ï¼Œä¸è¦æŒ‡å®š`--model_name_or_path`ï¼Œè¿™æ ·åªä¼šåŠ è½½æƒé‡ï¼Œè€Œä¸ä¼šåŠ è½½è®­ç»ƒçŠ¶æ€
- ä» ckpt ç»§ç»­è®­ç»ƒæ—¶ï¼Œloss å¯èƒ½ä¼šæœ‰å¾®å°çš„åå·®ï¼Œè¿™æ˜¯ç”±ä¸€äº›éç¡®å®šæ€§ç®—æ³•å¸¦æ¥çš„éšæœºæ€§ï¼Œæ˜¯æ­£å¸¸ç°è±¡ã€‚å‚è€ƒï¼š[HuggingFace Transformers Trainer Randomness 
- å½“ `--model_name_or_path` æœ‰æ•ˆæ—¶ï¼Œæ‰€æœ‰æ¨¡å‹ç›¸å…³çš„å‚æ•°éƒ½ä¼šè¢«å¿½ç•¥
- ä¸€ä¸ª batch å†…çš„æ ·æœ¬ä¼šé€šè¿‡ padding å¯¹é½ batch å†…æœ€é•¿çš„æ ·æœ¬ï¼Œè€Œæ¯æ¡æ ·æœ¬çš„é•¿åº¦æœ€é•¿ä¸º max_seq_lengthï¼Œè¶…å‡ºçš„éƒ¨åˆ†ä¼šè¢«è£å‰ª
- å¦‚æœæŠ¥å‡º bias æƒé‡æ²¡æœ‰ load çš„ warningï¼Œå¿½ç•¥å³å¯ï¼ŒHunyuan-Large ä¸­ä¸ä¼šç”¨åˆ° bias

#### æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

å‚è€ƒï¼š[DeepSpeed Configuration](https://www.deepspeed.ai/docs/config-json/)

å¯ä»¥å°è¯•ä¿®æ”¹ ds configï¼Œå»æ‰è¿™å‡ ä¸ªå‚æ•°çš„ auto å±æ€§ï¼Œæ”¹å°è¯•è¯•çœ‹ï¼š

- `stage3_param_persistence_threshold`
- `stage3_prefetch_bucket_size`
- `stage3_max_reuse_distance`


#### Lora æ¨¡å‹åˆå¹¶

ä¿å­˜ä¸‹æ¥çš„ lora æƒé‡æ²¡æ³•åœ¨è®­ç»ƒè¿è¡Œæ—¶åˆå¹¶åˆ° zero3 æ¨¡å‹ä¸­ï¼Œå› ä¸º zero3 å¼€å¯æ—¶æ¨¡å‹æƒé‡ä¼šåˆ‡åˆ†åˆ°å„ dp rank ä¸Šã€‚å› æ­¤å¦‚æœæƒ³æŠŠ lora æƒé‡åˆå¹¶åˆ° base æ¨¡å‹ä¸Šï¼Œå¯ä»¥é€šè¿‡ç¦»çº¿çš„æ–¹å¼åˆå¹¶åå¾—åˆ°æƒé‡æ–‡ä»¶ã€‚æ‰§è¡Œ`merge_lora_weight.sh`å³å¯å®Œæˆ lora æƒé‡å’Œ base æ¨¡å‹æƒé‡çš„åˆå¹¶ï¼Œå…¶ä¸­çš„å‚æ•°æœ‰ï¼š

- `--base_model_path`ï¼šbase æ¨¡å‹çš„æƒé‡ç›®å½•
- `--adapter_model_path`ï¼šlora æƒé‡ç›®å½•
- `--output_path`ï¼šåˆå¹¶åçš„æƒé‡ä¿å­˜ç›®å½•
- `--save_dtype`ï¼š ä»¥ä»€ä¹ˆæ•°æ®æ ¼å¼å­˜å‚¨åˆå¹¶åçš„æƒé‡ï¼Œå¯é€‰å€¼ï¼šfp16ï¼Œbf16ï¼Œfp32

#### LLaMA-Factory æ”¯æŒ

å¦‚æœå¯¹ LLaMA-Factory è¾ƒä¸ºç†Ÿæ‚‰ï¼Œå¯ä½¿ç”¨ https://github.com/hiyouga/LLaMA-Factory/tree/main è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬æä¾›äº† llama-factory çš„è®­ç»ƒç¤ºä¾‹é…ç½®æ–‡ä»¶ `./train/llama_factory_support/hunyuan_a13b_full_sft.yaml`æ–‡ä»¶ã€‚


&nbsp;

## Agent åŠŸèƒ½

Hunyuan-A13B æ¨¡å‹æ”¯æŒé€šè¿‡å‡½æ•°è°ƒç”¨ï¼ˆFunction Callï¼‰æ¥å®ç° Agent çš„æ­å»ºã€‚[Agentç¤ºä¾‹](agent/README.md)


## é‡åŒ–å‹ç¼©

æˆ‘ä»¬é‡‡ç”¨è‡ªç ”çš„`AngleSlim`å‹ç¼©å·¥å…·äº§å‡ºäº†FP8åŠINT4é‡åŒ–æ¨¡å‹ï¼Œ`AngleSlim`å‹ç¼©å·¥å…·é¢„è®¡7æœˆåˆå¼€æºï¼Œå°†æ”¯æŒå¤§æ¨¡å‹ä¸€é”®å¼é‡åŒ–å‹ç¼©ï¼Œæ•¬è¯·æœŸå¾…ï¼Œç°åœ¨å¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬çš„é‡åŒ–æ¨¡å‹è¿›è¡Œéƒ¨ç½²æµ‹è¯•ã€‚

### FP8é‡åŒ–
æˆ‘ä»¬é‡‡ç”¨`FP8-static`é‡åŒ–ï¼ŒFP8é‡åŒ–é‡‡ç”¨8ä½æµ®ç‚¹æ ¼å¼ï¼Œé€šè¿‡å°‘é‡æ ¡å‡†æ•°æ®ï¼ˆæ— éœ€è®­ç»ƒï¼‰é¢„å…ˆç¡®å®šé‡åŒ–scaleï¼Œå°†æ¨¡å‹æƒé‡ä¸æ¿€æ´»å€¼è½¬æ¢ä¸ºFP8æ ¼å¼ï¼Œæå‡æ¨ç†æ•ˆç‡å¹¶é™ä½éƒ¨ç½²é—¨æ§›ã€‚ 
æˆ‘ä»¬æ‚¨å¯ä»¥ä½¿ç”¨`AngleSlim`é‡åŒ–ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬é‡åŒ–å®Œæˆçš„å¼€æºæ¨¡å‹ä½¿ç”¨[Hunyuan-A13B-Instruct-FP8](https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8)ã€‚

#### FP8 Benchmark
æœ¬å°èŠ‚ä»‹ç» Hunyuan-A13B-Instruct-FP8 é‡åŒ–æ¨¡å‹çš„BenchmarkæŒ‡æ ‡ã€‚

|   Bench   | Hunyuan-A13B-Instruct | Hunyuan-A13B-Instruct-FP8 | 
|:---------:|:---------------------:|:-------------------------:|
| AIME 2024 |         87.3          |           86.7            |
|   Gsm8k   |         94.39         |           94.01           |
|    BBH    |         89.1          |           88.34           |
|   DROP    |         91.1          |           91.1            |



### Int4é‡åŒ–
Int4é‡åŒ–æˆ‘ä»¬é‡‡ç”¨[GPTQ](https://arxiv.org/abs/2210.17323 )ç®—æ³•å®ç°W4A16é‡åŒ–ï¼Œè¯¥ç®—æ³•é€å±‚å¤„ç†æ¨¡å‹æƒé‡ï¼Œåˆ©ç”¨å°‘é‡æ ¡å‡†æ•°æ®æœ€å°åŒ–é‡åŒ–åçš„æƒé‡é‡æ„è¯¯å·®ï¼Œé€šè¿‡è¿‘ä¼¼Hessiané€†çŸ©é˜µçš„ä¼˜åŒ–è¿‡ç¨‹é€å±‚è°ƒæ•´æƒé‡ã€‚æµç¨‹æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä»…éœ€å°‘é‡æ ¡å‡†æ•°æ®å³å¯é‡åŒ–æƒé‡ï¼Œæå‡æ¨ç†æ•ˆç‡å¹¶é™ä½éƒ¨ç½²é—¨æ§›ã€‚
æ‚¨å¯ä»¥ä½¿ç”¨`AngleSlim`é‡åŒ–ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½æˆ‘ä»¬é‡åŒ–å®Œæˆçš„å¼€æºæ¨¡å‹ä½¿ç”¨[Hunyuan-A13B-Instruct-Int4](https://huggingface.co/tencent/Hunyuan-A13B-Instruct-GPTQ-Int4)ã€‚

#### INT4 Benchmark
æœ¬å°èŠ‚ä»‹ç» Hunyuan-A13B-Instruct-GPTQ-Int4 é‡åŒ–æ¨¡å‹çš„BenchmarkæŒ‡æ ‡ã€‚

|     Bench      | Hunyuan-A13B-Instruct | Hunyuan-A13B-Instruct-GPTQ-Int4 | 
|:--------------:|:---------------------:|:-------------------------------:|
| OlympiadBench  |         82.7          |              84.0               |
|   AIME 2024    |         87.3          |              86.7               |
|     Gsm8k      |         94.39         |              94.24              |
|      BBH       |         88.34         |              87.91              |
|      DROP      |         91.12         |              91.05              |

&nbsp;

## æ¨ç†å’Œéƒ¨ç½² 

HunyuanLLMå¯ä»¥é‡‡ç”¨vLLMï¼Œsglangæˆ–TensorRT-LLMéƒ¨ç½²ã€‚ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹HunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒï¼Œè¯¦è§ã€ä½¿ç”¨vLLMæ¨ç†ã€‘ç« èŠ‚ã€‚



## ä½¿ç”¨vLLMæ¨ç†
### Docker:

ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ï¼ŒHunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒ (æ³¨æ„ï¼š è¯¥é•œåƒè¦æ±‚Hostçš„Cudaç‰ˆæœ¬ä¸º12.8ä»¥ä¸Šï¼‰ï¼š

[hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-vllm](https://hub.docker.com/r/hunyuaninfer/hunyuan-a13b/tags) ã€‚æ‚¨åªéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶ç”¨ä¸‹é¢ä»£ç å¯åŠ¨dockerå³å¯å¼€å§‹æ¨ç†æ¨¡å‹ã€‚
```shell
# ä¸‹è½½æ¨¡å‹ï¼š
# ModelScope: 
modelscope download --model Tencent-Hunyuan/Hunyuan-A13B-Instruct
# Huggingface: vllm ä¼šè‡ªåŠ¨ä¸‹è½½

# æ‹‰å–
docker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-vllm

# ä½¿ç”¨ huggingface èµ·æœåŠ¡
docker run  --privileged --user root  --net=host --ipc=host \
        -v ~/.cache:/root/.cache/ \
        --gpus=all -it --entrypoint python  hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-vllm \
         -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 \
         --tensor-parallel-size 4 --model tencent/Hunyuan-A13B-Instruct --trust-remote-code 

# ä½¿ç”¨modelscopeä¸‹è½½çš„æ¨¡å‹èµ·æœåŠ¡
docker run  --privileged --user root  --net=host --ipc=host \
        -v ~/.cache/modelscope:/root/.cache/modelscope \
        --gpus=all -it --entrypoint python   hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-vllm \
         -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --tensor-parallel-size 4 \
         --port 8000 --model /root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct/ --trust_remote_code           
```

æ³¨: Dockerå®¹å™¨æƒé™ç®¡ç†ã€‚ä»¥ä¸Šä»£ç é‡‡ç”¨ç‰¹æƒæ¨¡å¼ï¼ˆ--privilegedï¼‰å¯åŠ¨Dockerå®¹å™¨ä¼šèµ‹äºˆå®¹å™¨è¾ƒé«˜çš„æƒé™ï¼Œå¢åŠ æ•°æ®æ³„éœ²å’Œé›†ç¾¤å®‰å…¨é£é™©ã€‚å»ºè®®åœ¨éå¿…è¦æƒ…å†µä¸‹é¿å…ä½¿ç”¨ç‰¹æƒæ¨¡å¼ï¼Œä»¥é™ä½å®‰å…¨å¨èƒã€‚å¯¹äºå¿…é¡»ä½¿ç”¨ç‰¹æƒæ¨¡å¼çš„åœºæ™¯ï¼Œåº”è¿›è¡Œä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°ï¼Œå¹¶å®æ–½ç›¸åº”çš„å®‰å…¨ç›‘æ§ã€åŠ å›ºæªæ–½ã€‚


### BF16éƒ¨ç½²

BF16å¯ä»¥åœ¨2å¼ æ˜¾å­˜è¶…è¿‡80Gçš„GPUå¡ä¸Šéƒ¨ç½²ï¼Œå¦‚æœé•¿æ–‡æ¨èTP4ã€‚æŒ‰å¦‚ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

è¿è¡Œå‘½ä»¤å‰è¯·å…ˆè®¾ç½®å¦‚ä¸‹ç¯å¢ƒå˜é‡ï¼š

```shell
export MODEL_PATH=PATH_TO_MODEL
```

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`vLLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š

æ³¨: vLLMç»„ä»¶è¿œç¨‹ä»£ç æ‰§è¡Œé˜²æŠ¤ã€‚ä¸‹åˆ—ä»£ç ä¸­vLLMç»„ä»¶çš„trust-remote-codeé…ç½®é¡¹è‹¥è¢«å¯ç”¨ï¼Œå°†å…è®¸åŠ è½½å¹¶æ‰§è¡Œæ¥è‡ªè¿œç¨‹æ¨¡å‹ä»“åº“çš„ä»£ç ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¶æ„ä»£ç çš„æ‰§è¡Œã€‚é™¤éä¸šåŠ¡éœ€æ±‚æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™å»ºè®®è¯¥é…ç½®é¡¹å¤„äºç¦ç”¨çŠ¶æ€ï¼Œä»¥é™ä½æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚


```python
import os
from typing import List, Optional
from vllm import LLM, SamplingParams
from vllm.inputs import PromptType
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

llm = LLM(model=model_path,
        tokenizer=model_path,
        trust_remote_code=True,
        dtype='bfloat16',
        tensor_parallel_size=4,
        gpu_memory_utilization=0.9)

sampling_params = SamplingParams(
    temperature=0.7, top_p=0.8, max_tokens=4096, top_k=20, repetition_penalty=1.05)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]

tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")

dummy_inputs: List[PromptType] = [{
    "prompt_token_ids": batch
} for batch in tokenized_chat.numpy().tolist()]

outputs = llm.generate(dummy_inputs, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`vLLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å¹¶è¯·æ±‚

åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿è¡Œï¼š

```shell
export VLLM_HOST_IP=${LOCAL_IP}
```
æ¥ç€æˆ‘ä»¬å¯åŠ¨æœåŠ¡ï¼Œè¿è¡Œ :
```shell
cd inference
sh run_server.sh
```

è¿è¡Œ`run_server.sh`æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
sh openapi.sh
```

æ³¨æ„ä¿®æ”¹`openapi.sh`ä¸­çš„`${LOCAL_IP}`å’Œ`${MODEL_PATH}`ä¸ºæœåŠ¡å¯¹åº”å€¼ã€‚


### é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š

æœ¬éƒ¨åˆ†ä»‹ç»é‡‡ç”¨vLLMéƒ¨ç½²é‡åŒ–åæ¨¡å‹çš„æµç¨‹ã€‚

é•œåƒï¼šéƒ¨ç½²é•œåƒåŒBF16ã€‚


#### Int8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int8-weight-onlyç‰ˆæœ¬HunYuan-A13Bæ¨¡å‹åªéœ€è®¾ç½®`run_server_int8.sh`ä¸­çš„ç¯å¢ƒå˜é‡ï¼š
```SHELL
export MODEL_PATH=PATH_TO_BF16_MODEL
```

æ¥ç€æˆ‘ä»¬å¯åŠ¨Int8æœåŠ¡ã€‚è¿è¡Œï¼š
```shell
sh run_server_int8.sh
```

è¿è¡Œ`run_server_int8.sh`æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
sh openapi.sh
```

#### Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int4-weight-onlyç‰ˆæœ¬HunYuan-A13Bæ¨¡å‹åªéœ€è®¾ç½®`run_server_int4.sh`ä¸­çš„ç¯å¢ƒå˜é‡ï¼Œé‡‡ç”¨GPTQæ–¹å¼ï¼š
```SHELL
export MODEL_PATH=PATH_TO_INT4_MODEL
```

æ¥ç€æˆ‘ä»¬å¯åŠ¨Int4æœåŠ¡ã€‚è¿è¡Œï¼š
```shell
sh run_server_int4.sh
```

è¿è¡Œ`run_server_int4.sh`æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
sh openapi.sh
```

#### FP8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²W8A8C8ç‰ˆæœ¬HunYuan-A13Bæ¨¡å‹åªéœ€è®¾ç½®`run_server_int8.sh`ä¸­çš„ç¯å¢ƒå˜é‡ï¼š
```shell
export MODEL_PATH=PATH_TO_FP8_MODEL
```

æ¥ç€æˆ‘ä»¬å¯åŠ¨FP8æœåŠ¡ã€‚è¿è¡Œï¼š
```shell
sh run_server_fp8.sh
```

è¿è¡Œ`run_server_fp8.sh`æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
sh openapi.sh
```

### æ€§èƒ½è¯„ä¼°ï¼š

æœ¬éƒ¨åˆ†ä»‹ç»é‡‡ç”¨vLLMéƒ¨ç½²å„ä¸ªæ¨¡å‹ï¼ˆåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹ï¼‰çš„æ•ˆç‡æµ‹è¯•ç»“æœï¼ŒåŒ…æ‹¬ä¸åŒBatchsizeä¸‹çš„æ¨ç†é€Ÿåº¦(tokens/s), æµ‹è¯•ç¯å¢ƒï¼ˆè…¾è®¯äº‘ï¼ŒH80ï¼ˆ96Gï¼‰GPU x å¡æ•°ï¼‰:

æµ‹è¯•å‘½ä»¤ï¼š
```python
python3 benchmark_throughput.py --backend vllm \
         --input-len 2048 \
         --output-len 14336 \
         --model $MODEL_PATH \
         --tensor-parallel-size $TP \
         --use-v2-block-manager \
         --async-engine \
         --trust-remote-code \
         --num_prompts $BATCH_SIZE \
         --max-num-seqs $BATCH_SIZE
```

| æ¨ç†æ¡†æ¶ | æ¨¡å‹                          | éƒ¨ç½²å¡æ•°   | input_length | batch=1             | batch=16              | batch=32       |
|------|-----------------------------|-----------|-------------------------|---------------------|----------------------|----------------------|
| vLLM | Hunyuan-A13B-Instruct                   |    8     | 2048                  |      190.84     |       1246.54      |       1981.99     |
| vLLM | Hunyuan-A13B-Instruct                   |    4     | 2048                  |     158.90      |       779.10       |    1301.75        |
| vLLM | Hunyuan-A13B-Instruct                   |    2     | 2048                  |    111.72       |      327.31        |    346.54         |
| vLLM | Hunyuan-A13B-Instruct(int8 weight only) |    2      | 2048                  |   109.10       |      444.17        |     721.93        |
| vLLM | Hunyuan-A13B-Instruct(W8A8C8-FP8)       |    2      | 2048                  |    91.83       |      372.01        |      617.70       |
| vLLM | Hunyuan-A13B-Instruct(W8A8C8-FP8)       |    1      | 2048                  |     60.07      |         148.80     |      160.41       |


## ä½¿ç”¨TensorRT-LLMæ¨ç†

### BF16éƒ¨ç½²

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`TensorRT-LLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š
ä¿®æ”¹ examples/pytorch/quickstart_advanced.py ä¸­å¦‚ä¸‹ä»£ç ï¼š


```python
from tensorrt_llm import SamplingParams
from tensorrt_llm._torch import LLM
from tensorrt_llm._torch.pyexecutor.config import PyTorchConfig
from tensorrt_llm.llmapi import (EagleDecodingConfig, KvCacheConfig,
                                 MTPDecodingConfig)

prompt = "Write a short summary of the benefits of regular exercise"

def main():
    args = parse_arguments()

    llm, sampling_params = setup_llm(args)
    new_prompts = []
    if args.apply_chat_template:
        messages = [{"role": "user", "content": f"{prompt}"}]
        new_prompts.append(llm.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True)
        )

    outputs = llm.generate(new_prompts, sampling_params)

    for i, output in enumerate(outputs):
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"[{i}] Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

è¿è¡Œæ–¹å¼ï¼š

```shell
python3 quickstart_advanced.py --model_dir "HunyuanLLMæ¨¡å‹è·¯å¾„" --tp_size 4 --apply_chat_template
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`TensorRT-LLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

```shell
model_path="HunyuanLLMæ¨¡å‹è·¯å¾„"
trtllm-serve <model_path> [--backend pytorch --tp_size <tp> --ep_size <ep> --host <host> --port <port>]
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```python
### OpenAI Chat Client

from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="tensorrt_llm",
)

response = client.chat.completions.create(
    model="default",
    messages=[{
        "role": "user",
        "content": "Write a short summary of the benefits of regular exercise"
    }],
    max_tokens=4096,
)
print(response)
```

#### FP8/Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
ç›®å‰ TensorRT-LLM çš„ fp8 å’Œ int4 é‡åŒ–æ¨¡å‹æ­£åœ¨æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚


## ä½¿ç”¨sglangæ¨ç†

### BF16éƒ¨ç½²

#### Step1: æ‹‰å–é•œåƒ


```
docker pull tiacc-test.tencentcloudcr.com/tiacc/sglang:0.4.7
æˆ–
docker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-sglang
```

- å¯åŠ¨ API server:

```
docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    --ipc=host \
    tiacc-test.tencentcloudcr.com/tiacc/sglang:0.4.7 \
    -m sglang.launch_server --model-path hunyuan/huanyuan_A13B --tp 4 --trust-remote-code --host 0.0.0.0 --port 30000
```

#### Step2ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`sglang`å¿«é€Ÿè¯·æ±‚chat modelï¼š


```python
import sglang as sgl
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')


tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]
prompts = []
prompts.append(tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
))
print(prompts)

llm = sgl.Engine(
    model_path=model_path,
    tp_size=4,
    trust_remote_code=True,
    mem_fraction_static=0.7,
)

sampling_params = {"temperature": 0.7, "top_p": 0.8, "top_k": 20, "max_new_tokens": 4096}
outputs = llm.generate(prompts, sampling_params)
for prompt, output in zip(prompts, outputs):
    print(f"Prompt: {prompt}\nGenerated text: {output['text']}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`sglang`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

```shell
model_path="HunyuanLLMæ¨¡å‹è·¯å¾„"
python3 -u -m sglang.launch_server \
    --model-path $model_path \
    --tp 4 \
    --trust-remote-code \
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```python
import openai
client = openai.Client(
    base_url="http://localhost:30000/v1", api_key="EMPTY")

response = client.chat.completions.create(
    model="default",
    messages= [
        {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
    ],
    temperature=0.7,
    max_tokens=4096,
    extra_body={"top_p": 0.8, "top_k": 20}
)
print(response)
```

#### FP8/Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
ç›®å‰ sglang çš„ fp8 å’Œ int4 é‡åŒ–æ¨¡å‹æ­£åœ¨æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚

## äº¤äº’å¼Demo Web 
hunyuan-A13B ç°å·²å¼€æ”¾ç½‘é¡µdemoã€‚è®¿é—® https://hunyuan.tencent.com/?model=hunyuan-a13b å³å¯ç®€å•ä½“éªŒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

<br>

## å¼•ç”¨
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨æˆ‘ä»¬çš„<a href="report/Hunyuan_A13B_Technical_Report.pdf">æŠ€æœ¯æŠ¥å‘Š</a>ï¼

<br>


## è”ç³»æˆ‘ä»¬
å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬è…¾è®¯æ··å…ƒLLMå›¢é˜Ÿã€‚ä½ å¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆhunyuan_opensource@tencent.comï¼‰è”ç³»æˆ‘ä»¬ã€‚
